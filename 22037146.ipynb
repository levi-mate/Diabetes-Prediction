{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6adc6736-a94e-47a0-ade6-80822bea5fd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475748d-95de-4539-a8d4-9449bc58f737",
   "metadata": {},
   "source": [
    "In this assignment, I was given the task of implementing two machine learning models, that should give good predictions to cases of diabetes.\n",
    "\n",
    "I will go through all the important steps of the machine learning workflow, including treatment of the dataset, then selecting, training, applying and evaluating the models.\n",
    "\n",
    "I will explain the various steps I have taken during the process, and provide visualisations.\n",
    "\n",
    "Finally, I will compare the prediction results of the two models and draw sensible conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbc9fa-123c-4c20-a5f3-3f306ade4469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2e085-27bc-4ed9-a3db-8faaaa26ea35",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pre-processing the data is a critical phase in machine learning, as this will serve as the foundation for all the following steps.\n",
    "\n",
    "Various actions can and should be taken when dealing with the dataset, which usually depends on the goal of the project.\n",
    "\n",
    "Handling missing or 0 values, addressing inaccuracies and inconsistencies and various other tasks can seriously affect the outcomes.\n",
    "\n",
    "If these steps are not taken, the model can become biased and distorted and the outputs would become unreliable and even dangerous in some cases.\n",
    "\n",
    "To address these issues, I will treat the data with some well tested methods.\n",
    "\n",
    "First, the dataset is loaded into a dataframe using pandas.\n",
    "\n",
    "Using the dataframe, we can observe the first few entries and column names. This helps with understanding the data's features and prepares us for the necessary preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cccfd-7c59-4899-98aa-2672d47c0f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60047c36-7883-43dc-8813-862f1815d2c4",
   "metadata": {},
   "source": [
    "We can see the tendencies and shape of the dataset's distribution. \n",
    "\n",
    "This step is crucial to identify anomalies and understand the scale of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e55eeb-e875-483f-a1c8-fde7a9b5d443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ac52b-c5dd-4663-84b8-e84292a01e64",
   "metadata": {},
   "source": [
    "Here, we can see the data types and non-null counts, it helps us determine the cleanliness of the data and the potential need for type conversions and handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa9d55-6ff8-4da5-ac77-738d4aa46956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647a317-aced-4c0b-b1b9-0ecae4c57a63",
   "metadata": {},
   "source": [
    "Understanding the balance of the classes is very important.\n",
    "\n",
    "Real world datasets are often imbalanced, in which case we have to take measures to prevent inaccuracies in the output.\n",
    "\n",
    "When one of the classes is overrepresented, it can create a bias towards it, which is a serious problem when it comes to predicting cases of medical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562bb9c-448d-465d-bfe2-1451d597f061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f6f20-952b-4996-a44c-a868af74249f",
   "metadata": {},
   "source": [
    "Visualising the data provides insights into the distribution, correlations, and patterns within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184ee30-2db4-4afd-a299-44dc03f67f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df.hist(figsize = (12, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.plot(kind = 'box', subplots = True, layout = (3,3), figsize = (12, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(df.corr(), annot = True, fmt = \".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e5249-fe45-4907-8820-3aefe549abf0",
   "metadata": {},
   "source": [
    "As we saw earlier, some of the columns have zero values.\n",
    "\n",
    "To address the issue, we impute them with median values as a reasonable estimation.\n",
    "\n",
    "The only columns we impute are the ones where it makes sense, and leave others out like \"pregnancies\".\n",
    "\n",
    "This approach mitigates the skewing effect outliers can have on mean imputation, thus preserving the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7572e-eddb-466a-b1d1-ca6fd8941b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for col in col_to_impute:\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "X = df.drop(['Outcome'], axis = 1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67d865-3137-467a-993b-112ec616fbe7",
   "metadata": {},
   "source": [
    "The next step is one of the most important data processing steps in machine learning.\n",
    "\n",
    "Right now, the dataset is in one piece, and training the model on it would be ill advised.\n",
    "\n",
    "After training, we would have no choice but to test the model on the same data it was trained on.\n",
    "\n",
    "This would mean that the model would suffer from overfitting, as it would have seen the data during training and knew the correct labels already.\n",
    "\n",
    "To prevent that, we split the dataset into a training set and a test set following a conventional 80-20 ratio.\n",
    "\n",
    "This widely accepted practice allows us to train our models on the majority of the data while setting aside a portion for unbiased evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f522ee-213d-486c-9007-704e85febd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561649c-3bbd-4565-8fd3-ea2e9d65e14c",
   "metadata": {},
   "source": [
    "Now that dataset is properly split, but as we saw earlier, the various data fields have different scales.\n",
    "\n",
    "To ensure that each feature contributes proportionally to the final model, we need to scale the data.\n",
    "\n",
    "Standardization modifies the features to have a mean of zero and a standard deviation of one, which is particularly beneficial for algorithms sensitive to the scale of the data, such as SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fe90b-ac90-4407-992d-93ff6358ee36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9afa0c-51a7-453a-8b39-f9f564a51339",
   "metadata": {},
   "source": [
    "Finally, we implement Stratified K-Folds, for use in the grid search later.\n",
    "\n",
    "The purpose of this is to make sure that each fold of the dataset contains the same percentage of samples of the classes, which is important in our case.\n",
    "\n",
    "In this case, we are using 5 folds for cross validation, which means 4 folds will be used for training and 1 for validations, repeating 5 times.\n",
    "\n",
    "This method is beneficial for maintaining a representative training process, especially when the dataset shows an imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497f4be-5d18-4afa-a89d-6d225be4ce87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb8438-2143-4c21-af68-6858e4fbb20a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f640b-9c74-4bd5-a33a-ed8f588c6699",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a type of supervised learning used for classification and regression. The algorithm works by finding the hyperplane (decision boundary) that best divides a dataset into classes.\n",
    "\n",
    "The best hyperplane for an SVM means the one with the largest margin between the two classes. Margins are a gap between the two lines on the closest class points. This is why SVM is also known as a margin classifier.\n",
    "\n",
    "The larger the margin between these points (support vectors), the lower the error of the classifier.\n",
    "\n",
    "Linear SVM: When the data is linearly separable, the algorithm finds a hyperplane that separates the classes with the maximum margin.\n",
    "\n",
    "Non-linear SVM: When the dataset cannot be separated linearly, SVM uses a kernel trick to transform the input space into a higher-dimensional space where a hyperplane can be used to separate the classes. \n",
    "\n",
    "The kernel trick uses kernels (e.g. polynomial, RBF), which compute the high-dimensional relationships without having to actually do the transformation.\n",
    "\n",
    "Hyperparameters:\n",
    "- `C` (for all kernels): gives weight to classification error, larger values give more importance to the classification error.\n",
    "- `gamma` (for RBF and polynomial kernels): defines the reach of an individual training example's influence, essential for non-linear kernel functions.\n",
    "- `degree (d)` (for polynomial kernel): determines the flexibility of the decision boundary, enabling it to take on more complex shapes.\n",
    "- `coef0 (r)` (for polynomial kernel): adjusts the model's sensitivity to higher-order features.\n",
    "\n",
    "The selection and tuning of hyperparameters is very important, as they can greatly influence the model's ability to classify new data correctly.\n",
    "\n",
    "We optimise these parameters through grid search with cross-validation to find the most effective combination of these parameters, especially to achieve a high recall.\n",
    "\n",
    "We focus on recall, because that is the metric that show how often the model correctly identifies positive cases.\n",
    "\n",
    "Recall is often prioritised in medicine, because false negatives have serious consequences.\n",
    "\n",
    "If a patient is diagnosed as a false negative, then they might go on believing that they do not have the condition, which can have serious consequences for their health.\n",
    "\n",
    "If they are diagnosed as a false positive however, then they might have some anxiety for a period of time about it, but ultimately they do not suffer from diabetes so falsely believing that they do is not a big issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb99a79-8c73-4ffc-8a70-ca81578647bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "svm_start = time.time()\n",
    "\n",
    "svm_parameters = [\n",
    "    {'kernel': ['linear'], 'C': [0.0001, 0.001, 0.1, 1, 10, 100]},\n",
    "    {'kernel': ['rbf'], 'C': [0.0001, 0.001, 0.1, 1, 10, 100], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1]},\n",
    "    {'kernel': ['poly'], 'C': [0.0001, 0.001, 0.1, 1, 10, 100], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1], 'degree': [2, 3], 'coef0': [0.0, 1.0]}\n",
    "]\n",
    "\n",
    "print(\"Conductiong grid search, please wait...\\n\")\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(class_weight = 'balanced'), svm_parameters, cv = stratified_kfold, scoring = 'recall', n_jobs = -1)\n",
    "\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "svm_end = time.time()\n",
    "\n",
    "svm_grid_duration = svm_end - svm_start\n",
    "\n",
    "print(f\"SVM grid search took: {svm_grid_duration} seconds to complete\\n\")\n",
    "\n",
    "print(\"Best Parameters: \", svm_grid_search.best_params_)\n",
    "\n",
    "print(\"\\nEnd of SVM grid search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abdb46-af31-426e-8dae-70c97f0aa74e",
   "metadata": {},
   "source": [
    "With the optimal hyperparameters found by the grid search, we can create a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba247b0-c73e-4940-9aeb-88b7f5b11ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_svm = SVC(**svm_grid_search.best_params_)\n",
    "best_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d6dac-0140-45df-a358-17b7d1fe34be",
   "metadata": {},
   "source": [
    "Using the trained SVM model, we generate predictions on the test set.\n",
    "\n",
    "These predictions allow us to critically evaluate the model's performance on unseen data, which is essential for gauging how the model might perform in real world scenarios.\n",
    "\n",
    "We assess the model's effectiveness using a variety of metrics:\n",
    "- Accuracy: measures the overall correctness of the model.\n",
    "- Precision: measures the proportion of correctly predicted positive instances.\n",
    "- Recall: measures the completeness of positive predictions.\n",
    "- F1: provides a balance between precision and recall, important when we need a single metric to convey the balance between these two metrics.\n",
    "\n",
    "The evaluation phase is not just about assessing model accuracy, it's about understanding the model's predictive behavior.\n",
    "\n",
    "We utilise a confusion matrix to visualise true and false positives and negatives.\n",
    "\n",
    "Such analyses are crucial for medical decision making where the cost of a false negative can be much higher than that of a false positive, as previously mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab5e4a-ea2c-4141-af67-9df88428ddea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "con_mat_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(\"\\nSVM accuracy:\", accuracy_svm)\n",
    "print(\"\\nSVM classification report:\\n\", class_report_svm)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(con_mat_svm, annot = True, fmt = 'd', cmap = 'Blues', xticklabels = ['Not Diabetic', 'Diabetic'], yticklabels=['Not Diabetic', 'Diabetic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32f992-1db8-4c0e-aaa1-68f5a72cb13b",
   "metadata": {},
   "source": [
    "The confusion matrix provides a visualisation of the model's predictions. We can see that out of 154 instances, the model correctly predicted 88 non-diabetic cases and 29 diabetic cases, giving us an accuracy of about 76%. \n",
    "\n",
    "However, more critical for us is the number of false negatives. There were 26 such cases, which is significant in medical diagnoses where failing to identify the condition can have serious consequences.\n",
    "\n",
    "Looking at the classification report, we see the model's precision, recall, and f1-scores.\n",
    "\n",
    "The recall for detecting diabetic cases stands at 53%, which implies that the model is not sensitive enough to positive cases.\n",
    "\n",
    "The weighted average f1-score, which combines precision and recall, is 75%, suggesting a balance between the precision and recall across the classes.\n",
    "\n",
    "This balance is crucial since it ensures that our model does not overly favor one class over the other.\n",
    "\n",
    "In summary, the SVM model demonstrates a decent performance. However, using another type of machine learning model might give us better results for our particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060bc8a-21f0-43a2-a65e-17d01cf3eb65",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebbc0b0-9783-4def-b85f-d4d7e6871a1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ensembles are a method in machine learning that involve combining several models to improve the accuracy of predictions.\n",
    "\n",
    "These methods work on the assumption that a group of weak learners can come together to form a stronger learner.\n",
    "\n",
    "The advantage of using them is that they help in reducing overfitting and bias, and can lead to better performance compared to using a single model.\n",
    "\n",
    "### AdaBoost with RandomForest\n",
    "\n",
    "Adaptive Boosting (AdaBoost), builds a model from the training data, then creates a second model that tries to correct the errors from the first model.\n",
    "\n",
    "The process is repeated, with adding models until the training data is predicted well or a maximum number of models are added.\n",
    "\n",
    "RandomForest is an algorithm that builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "\n",
    "It is known for high performance and scalability.\n",
    "\n",
    "Each tree in a RandomForest is built from a sample drawn with replacement from the training set.\n",
    "\n",
    "When splitting a node during the construction of the tree, the split chosen is no longer the best split among all the features.\n",
    "\n",
    "Instead, the split which is picked is the best split among a random subset of the features.\n",
    "\n",
    "As a result, the bias of the forest increases slightly but due to averaging, its variance decreases, which results in an overall better model.\n",
    "\n",
    "For this task, the AdaBoost algorithm is used with RandomForest as the base classifier, and these are the chosen parameters:\n",
    "- `base_estimator__max_depth`: maximum depth of the trees. Deeper trees can capture more complex patterns but also lead to overfitting.\n",
    "- `base_estimator__min_samples_split`: minimum number of samples required to split an internal node.\n",
    "- `base_estimator__n_estimators`: number of trees in the RandomForest. More trees can give us better performance but also require more resources.\n",
    "- `n_estimators` for AdaBoost: maximum number of estimators at which boosting is terminated.\n",
    "- `learning_rate`: shrinks the contribution of the classifiers. There is a trade-off between the learning_rate and n_estimators.\n",
    "\n",
    "I chose these parameters because of their control over bias and variance trade off, learning speed and complexity.\n",
    "\n",
    "Just like we did with the SVM model, we perform a grid search with cross-validation, focusing on achieving the best recall we can, while also measuring computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd822463-a808-4102-aa73-e126d5a1d6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "ada_start = time.time()\n",
    "\n",
    "rnd_clf = RandomForestClassifier(class_weight = 'balanced', random_state = 42)\n",
    "\n",
    "ada_classifier = AdaBoostClassifier(rnd_clf, random_state = 42, algorithm = 'SAMME')\n",
    "\n",
    "ada_parameters = {\n",
    "    'base_estimator__max_depth': [5, 10, None],\n",
    "    'base_estimator__min_samples_split': [2, 4],\n",
    "    'base_estimator__n_estimators': [10, 50, 100],\n",
    "    'n_estimators': [30, 50],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "print(\"Conductiong grid search, please wait...\\n\")\n",
    "\n",
    "ada_grid_search = GridSearchCV(ada_classifier, param_grid = ada_parameters, cv = stratified_kfold, scoring = 'recall', n_jobs = -1)\n",
    "\n",
    "ada_grid_search.fit(X_train, y_train)\n",
    "\n",
    "ada_end = time.time()\n",
    "\n",
    "ada_grid_duration = ada_end - ada_start\n",
    "\n",
    "print(f\"Ensemble grid search took: {ada_grid_duration} seconds to complete\\n\")\n",
    "\n",
    "print(\"Best Parameters: \", ada_grid_search.best_params_)\n",
    "\n",
    "print(\"\\nEnd of ensemble grid search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0735dcc-2158-45b9-8e6f-b747573703af",
   "metadata": {},
   "source": [
    "After finding the best values from the grid search, we can now see the best model.\n",
    "\n",
    "This model is expected to outperform the individual weak learners, giving us better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752d158-4eda-4099-91e2-e8e39a7f1016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_ada = ada_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165dabe5-7b1b-47aa-8f61-8e9347acdf65",
   "metadata": {},
   "source": [
    "After training, we can now predict on the test set.\n",
    "\n",
    "We use the same metrics for the results as we did with the previous SVM model, including accuracy, precision, recall, f1 and a visual confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b1458-af0a-432a-b571-d155fc47d8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_ada = best_ada.predict(X_test)\n",
    "\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "con_mat_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "class_report_ada = classification_report(y_test, y_pred_ada)\n",
    "\n",
    "print(\"AdaBoost with RandomForest Accuracy:\", accuracy_ada)\n",
    "print(\"AdaBoost with RandomForest Classification Report:\\n\", class_report_ada)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(con_mat_ada, annot = True, fmt = 'd', cmap = 'Blues', xticklabels = ['Not Diabetic', 'Diabetic'], yticklabels = ['Not Diabetic', 'Diabetic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6f756-74ff-41f5-a2a3-ffe695f2e7cf",
   "metadata": {},
   "source": [
    "The results from the final ensemble model shows us balanced performance over different metrics.\n",
    "\n",
    "The model achieved an accuracy of around 75%. The true value of the model however is seen in the other metrics that accounts for the actual use case of diabetes prediction.\n",
    "\n",
    "The confusion matrix shows that the model predicted 70 non-diabetic and 45 diabetic cases correctly.\n",
    "\n",
    "There are 29 instances of false positives and much more importantly, 10 false negatives, which as I have previously discussed, is the primary focus in our project.\n",
    "\n",
    "Precision for non-diabetic predictions is 88%, showing a high likelihood that a non-diabetic prediction by the model is correct.\n",
    "\n",
    "Recall for diabetic predictions is 82%, which is particularly important. While it's not perfect, it means the model is quite reliable at catching positive cases.\n",
    "\n",
    "F1, which balances precision and recall, is at 78% for non-diabetic and 70% for diabetic predictions.\n",
    "\n",
    "These suggest that the model is better at identifying non-diabetic cases but still provides a good performance on diabetic cases.\n",
    "\n",
    "In conclusion, the model performs well, especially when considering our case of diabetes prediction.\n",
    "\n",
    "The number of false negatives is kept low, and while there are a decent number of false positives, in a medical setting these would simply mean that the patients would be sent in for further testing to determine their true condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30639276-3d54-47dd-82b2-cd7ee679fe89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe71b6a-96ca-4f7f-a020-9e5728b5c7e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The purpose of this assignment is to implement, tune and analyse two different machine learning models in order to create models that can effectively predict cases of diabetes.\n",
    "\n",
    "The two models I have implemented are a Support Vector Machine (SVM) and an ensemble using an AdaBoost with RandomForest approach.\n",
    "\n",
    "While both models aim to provide good predictions, their method of approaching the classification problem is different, leading to a differences in their performance and effectiveness for this task.\n",
    "\n",
    "### SVM:\n",
    "The SVM model displayed good accuracy of about 76%, indicating a decent ability in distinguishing between the classes.\n",
    "\n",
    "Its strength lies in its precision for non-diabetic cases at 77%, along with a mediocre recall for diabetic class at 53%.\n",
    "\n",
    "However, the model's sensitivity to diabetes detection is somewhat concerning, because it reflects the potential risk of misclassifying people with diabetes as non-diabetic, which could have serious consequences.\n",
    "\n",
    "### AdaBoost with RandomForest:\n",
    "The AdaBoost with RandomForest ensemble model shows us a similar accuracy of about 75%.\n",
    "\n",
    "Its recall for diabetic cases is significantly higher than the SVM at 82%, highlighting the ensemble's ability to identify the critical positive cases.\n",
    "\n",
    "This is vital in medical diagnoses, as it reduces the risk of overlooking patients in need of medical care.\n",
    "\n",
    "### Advantages and Disadvantages:\n",
    "The SVM model, with its kernel trick and support vectors, is good at finding the hyperplane for classification tasks and works well with smaller datasets.\n",
    "\n",
    "However, it can be less effective when the dataset is not linearly separable and may require careful tuning of hyperparameters to balance results.\n",
    "\n",
    "The AdaBoost with RandomForest model shows the strength of multiple decision trees, reducing variance and bias by averaging the results, which is why it performs better in our case.\n",
    "\n",
    "Its disadvantage is in its complexity and the potential for overfitting if not properly tuned, though this is somewhat mitigated by the RandomForest's capacity for generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c1c95-ea81-44e1-a7ad-6e3dc5ff6e14",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Considering the importance of minimising false negatives in predicting diabetes, the AdaBoost with RandomForest model stands out as the more suitable model.\n",
    "\n",
    "Its high recall rate means that fewer diabetic cases go undetected, which is the primary objective of this application of the model.\n",
    "\n",
    "Although SVM offers a strong baseline, the ensemble offers a better understanding of the data, proving to be more effective for this specific medical diagnosis task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
